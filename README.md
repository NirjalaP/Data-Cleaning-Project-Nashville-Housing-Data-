Executive Summary

This project focuses on cleaning and transforming the Nashville Housing dataset using T-SQL in SQL Server. The original dataset contained inconsistent formats, missing values, duplicate records, and unstructured address fields.

The objective was to convert the raw dataset into a clean, structured, and analysis-ready table suitable for reporting, visualization, and real estate market analysis.

Business Problem

Raw real-estate datasets often contain inconsistencies that make analysis unreliable or time-consuming. In this dataset:

‚Ä¢ Dates were stored as text instead of proper date format
‚Ä¢ Property addresses were missing in several records
‚Ä¢ Address fields were stored as a single string instead of structured columns
‚Ä¢ Boolean/categorical values were inconsistent (Y/N vs Yes/No)
‚Ä¢ Duplicate property sales existed
‚Ä¢ Several columns were redundant after transformation

Without cleaning, analysts could:

Misinterpret time trends

Double-count property sales

Struggle with geographic analysis

Spend unnecessary time preparing data instead of analyzing it

The business need was to create a reliable, analytics-ready dataset.

Methodology
1Ô∏è‚É£ Standardize Date Format

Created a new column SaleDateConverted and converted the original SaleDate into SQL DATE format.

This enables:

Time series analysis

Monthly/quarterly grouping

Accurate filtering and reporting

2Ô∏è‚É£ Populate Missing Property Addresses

Used a self-join on ParcelID to fill missing PropertyAddress values from other rows representing the same property.

This step reduced null values and preserved data completeness.

3Ô∏è‚É£ Split Property Address into Structured Columns

Extracted:

Street Address ‚Üí PropertySplitAddress

City ‚Üí PropertySplitCity

Used string parsing functions:

SUBSTRING()

CHARINDEX()

This enables location-based analysis and easier filtering.

4Ô∏è‚É£ Split Owner Address into Address, City, State

Created:

OwnerSplitAddress

OwnerSplitCity

OwnerSplitState

Used:

REPLACE()

PARSENAME()

This converts an unstructured text field into structured geographic attributes.

5Ô∏è‚É£ Standardize Categorical Values

Converted inconsistent values in SoldAsVacant:

Y ‚Üí Yes

N ‚Üí No

Ensures clean grouping and reporting.

6Ô∏è‚É£ Identify Duplicate Records

Used a CTE + ROW_NUMBER() window function to detect duplicates based on:

ParcelID

PropertyAddress

SalePrice

SaleDate

LegalReference

This prevents double-counting in future analytics.

7Ô∏è‚É£ Remove Redundant Columns

Dropped columns no longer needed after transformation:

OwnerAddress

TaxDistrict

PropertyAddress

SaleDate

This produces a cleaner, more efficient dataset.

Skills Demonstrated

SQL Server ‚Ä¢ T-SQL
Data Cleaning & Transformation
Data Quality Improvement
Window Functions (ROW_NUMBER)
Self-Joins for Data Imputation
String Parsing Functions
Schema Modification (ALTER / DROP)
Analytical Data Preparation

Result / Solution

The dataset was transformed from a raw operational dataset into an analysis-ready table:

‚úî Standardized date format
‚úî Missing addresses populated
‚úî Addresses converted into structured columns
‚úî Categorical values standardized
‚úî Duplicate records identified
‚úî Redundant columns removed

The final dataset is now ready for:

Power BI / Tableau dashboards

Real estate trend analysis

Predictive modeling

Geographic analysis

Next Steps

Future improvements that could extend this project:

‚Ä¢ Create a data quality dashboard to monitor nulls & duplicates
‚Ä¢ Build a real estate price trend dashboard (Power BI / Tableau)
‚Ä¢ Add ZIP code / geospatial enrichment
‚Ä¢ Automate the cleaning pipeline using SQL jobs
‚Ä¢ Build staging and production tables for ETL workflow

Impact

This cleaning process:

üìâ Reduced data inconsistencies
üìä Improved data reliability for analytics
‚è± Saved analyst time spent on data preparation
üìç Enabled geographic and time-based insights
üìà Created a strong foundation for BI and advanced analytics
